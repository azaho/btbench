# ðŸ§ª Evaluation Submissions

To submit a model evaluation, follow these steps:

1. Fork this repository.
2. Clone your fork to your local machine.
3. Create a new folder under `evaluation/<task_name>/` with your submission date and 
model name (e.g., `20250211_my_decoder`).
4. Include the following **required** files:
   - `all_preds.json`: Model predictions.
   - `metadata.yaml`: Metadata with evaluation details.
   - `README.md`: Brief description of your submission.
   - `logs/`: Any logs produced during evaluation.

5. Open a Pull Request (PR) to submit your results.
6. Your submission will be reviewed and added to the leaderboard.

---
*For more details, check `checklist.md`.*

