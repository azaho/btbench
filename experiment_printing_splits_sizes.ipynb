{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected braintreebank data at: /om2/user/zaho/braintreebank/braintreebank\n",
      "Sampling rate: 2048 Hz\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import btbench_config\n",
    "# Make sure the config ROOT_DIR is set correctly\n",
    "print(\"Expected braintreebank data at:\", btbench_config.ROOT_DIR)\n",
    "print(\"Sampling rate:\", btbench_config.SAMPLING_RATE, \"Hz\")\n",
    "from braintreebank_subject import BrainTreebankSubject\n",
    "from btbench_datasets import BrainTreebankSubjectTrialBenchmarkDataset\n",
    "\n",
    "btbench_tasks = [\"frame_brightness\", \"global_flow\", \"local_flow\", \"global_flow_angle\", \"local_flow_angle\", \"face_num\", \"volume\", \"pitch\", \"delta_volume\", \n",
    "                    \"delta_pitch\", \"speech\", \"onset\", \"gpt2_surprisal\", \"word_length\", \"word_gap\", \"word_index\", \"word_head_pos\", \"word_part_speech\", \"speaker\"]\n",
    "\n",
    "btbench_tasks = [\"frame_brightness\", \"global_flow\", \"local_flow\", \"face_num\", \"volume\", \"pitch\", \"delta_volume\", \n",
    "                    \"delta_pitch\", \"speech\", \"onset\", \"gpt2_surprisal\", \"word_length\", \"word_gap\", \"word_index\", \"word_head_pos\", \"word_part_speech\", \"speaker\"]\n",
    "\n",
    "#btbench_tasks = [\"enhanced_pitch\"]#, \"enhanced_volume\", \"delta_enhanced_pitch\", \"delta_enhanced_volume\", \"raw_pitch\", \"raw_volume\", \"delta_raw_pitch\", \"delta_raw_volume\"]\n",
    "\n",
    "all_subject_trials = [(1, 0), (1, 1), (1, 2), (2, 0), (2, 1), (2, 2), (2, 3), (2, 4), (2, 5), (2, 6), (3, 0), (3, 1), (3, 2), (4, 0), (4, 1), (4, 2), (5, 0), (6, 0), (6, 1), (6, 4), (7, 0), (7, 1), (8, 0), (9, 0), (10, 0), (10, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Subject 1, Trial 0 ===\n",
      "Number of electrodes: 130\n",
      "Movie name: fantastic-mr-fox\n",
      "\n",
      "Task sizes:\n",
      "  frame_brightness: 3953 items\n",
      "  global_flow: 3952 items\n",
      "  local_flow: 3952 items\n",
      "  face_num: 3638 items\n",
      "  volume: 3953 items\n",
      "  pitch: 3953 items\n",
      "  delta_volume: 3953 items\n",
      "  delta_pitch: 3953 items\n",
      "  speech: 4928 items\n",
      "  onset: 2558 items\n",
      "  gpt2_surprisal: 3953 items\n",
      "  word_length: 3937 items\n",
      "  word_gap: 4003 items\n",
      "  word_index: 2558 items\n",
      "  word_head_pos: 7072 items\n",
      "  word_part_speech: 2424 items\n",
      "  speaker: 6918 items\n",
      "\n",
      "=== Subject 1, Trial 1 ===\n",
      "Number of electrodes: 130\n",
      "Movie name: the-martian\n",
      "\n",
      "Task sizes:\n",
      "  frame_brightness: 5395 items\n",
      "  global_flow: 5394 items\n",
      "  local_flow: 5395 items\n",
      "  face_num: 7554 items\n",
      "  volume: 5395 items\n",
      "  pitch: 5395 items\n",
      "  delta_volume: 5394 items\n",
      "  delta_pitch: 5395 items\n",
      "  speech: 12818 items\n",
      "  onset: 3134 items\n",
      "  gpt2_surprisal: 5395 items\n",
      "  word_length: 5393 items\n",
      "  word_gap: 4773 items\n",
      "  word_index: 3134 items\n",
      "  word_head_pos: 9332 items\n",
      "  word_part_speech: 3332 items\n",
      "  speaker: 6186 items\n",
      "\n",
      "=== Subject 1, Trial 2 ===\n",
      "Number of electrodes: 130\n",
      "Movie name: thor-ragnarok\n",
      "\n",
      "Task sizes:\n",
      "  frame_brightness: 4207 items\n",
      "  global_flow: 4208 items\n",
      "  local_flow: 4206 items\n",
      "  face_num: 3818 items\n",
      "  volume: 4207 items\n",
      "  pitch: 4207 items\n",
      "  delta_volume: 4207 items\n",
      "  delta_pitch: 4207 items\n",
      "  speech: 6442 items\n",
      "  onset: 2916 items\n",
      "  gpt2_surprisal: 4207 items\n",
      "  word_length: 4207 items\n",
      "  word_gap: 3518 items\n",
      "  word_index: 2916 items\n",
      "  word_head_pos: 7590 items\n",
      "  word_part_speech: 2648 items\n",
      "  speaker: 6258 items\n",
      "\n",
      "=== Subject 2, Trial 0 ===\n",
      "Number of electrodes: 135\n",
      "Movie name: venom\n",
      "\n",
      "Task sizes:\n",
      "  frame_brightness: 3569 items\n",
      "  global_flow: 3570 items\n",
      "  local_flow: 3567 items\n",
      "  face_num: 4308 items\n",
      "  volume: 3569 items\n",
      "  pitch: 3569 items\n",
      "  delta_volume: 3569 items\n",
      "  delta_pitch: 3569 items\n",
      "  speech: 8120 items\n",
      "  onset: 2632 items\n",
      "  gpt2_surprisal: 3569 items\n",
      "  word_length: 3569 items\n",
      "  word_gap: 3038 items\n",
      "  word_index: 2632 items\n",
      "  word_head_pos: 6604 items\n",
      "  word_part_speech: 2290 items\n",
      "  speaker: 5380 items\n",
      "\n",
      "=== Subject 2, Trial 1 ===\n",
      "Number of electrodes: 135\n",
      "Movie name: spider-man-3-homecoming\n",
      "\n",
      "Task sizes:\n",
      "  frame_brightness: 5677 items\n",
      "  global_flow: 5677 items\n",
      "  local_flow: 5677 items\n",
      "  face_num: 7834 items\n",
      "  volume: 5677 items\n",
      "  pitch: 5677 items\n",
      "  delta_volume: 5677 items\n",
      "  delta_pitch: 5677 items\n",
      "  speech: 6674 items\n",
      "  onset: 4334 items\n",
      "  gpt2_surprisal: 5677 items\n",
      "  word_length: 5681 items\n",
      "  word_gap: 4818 items\n",
      "  word_index: 4334 items\n",
      "  word_head_pos: 10514 items\n",
      "  word_part_speech: 3540 items\n",
      "  speaker: 4016 items\n",
      "\n",
      "=== Subject 2, Trial 2 ===\n",
      "Number of electrodes: 135\n",
      "Movie name: guardians-of-the-galaxy\n",
      "\n",
      "Task sizes:\n",
      "  frame_brightness: 3907 items\n",
      "  global_flow: 3907 items\n",
      "  local_flow: 3907 items\n",
      "  face_num: 6050 items\n",
      "  volume: 3907 items\n",
      "  pitch: 3907 items\n",
      "  delta_volume: 3907 items\n",
      "  delta_pitch: 3907 items\n",
      "  speech: 11400 items\n",
      "  onset: 2318 items\n",
      "  gpt2_surprisal: 3907 items\n",
      "  word_length: 3911 items\n",
      "  word_gap: 3517 items\n",
      "  word_index: 2318 items\n",
      "  word_head_pos: 7052 items\n",
      "  word_part_speech: 2472 items\n",
      "  speaker: 4058 items\n",
      "\n",
      "=== Subject 2, Trial 3 ===\n",
      "Number of electrodes: 135\n",
      "Movie name: guardians-of-the-galaxy-2\n",
      "\n",
      "Task sizes:\n",
      "  frame_brightness: 4323 items\n",
      "  global_flow: 4322 items\n",
      "  local_flow: 4322 items\n",
      "  face_num: 6110 items\n",
      "  volume: 4323 items\n",
      "  pitch: 4323 items\n",
      "  delta_volume: 4323 items\n",
      "  delta_pitch: 4323 items\n",
      "  speech: 16264 items\n",
      "  onset: 2500 items\n",
      "  gpt2_surprisal: 4323 items\n",
      "  word_length: 4378 items\n",
      "  word_gap: 4096 items\n",
      "  word_index: 2500 items\n",
      "  word_head_pos: 7604 items\n",
      "  word_part_speech: 2644 items\n",
      "  speaker: 3592 items\n",
      "\n",
      "=== Subject 2, Trial 4 ===\n",
      "Number of electrodes: 135\n",
      "Movie name: avengers-infinity-war\n",
      "\n",
      "Task sizes:\n",
      "  frame_brightness: 3961 items\n",
      "  global_flow: 3962 items\n",
      "  local_flow: 3961 items\n",
      "  face_num: 7360 items\n",
      "  volume: 3961 items\n",
      "  pitch: 3961 items\n",
      "  delta_volume: 3961 items\n",
      "  delta_pitch: 3961 items\n",
      "  speech: 13060 items\n",
      "  onset: 2992 items\n",
      "  gpt2_surprisal: 3961 items\n",
      "  word_length: 3962 items\n",
      "  word_gap: 3226 items\n",
      "  word_index: 2992 items\n",
      "  word_head_pos: 7264 items\n",
      "  word_part_speech: 2588 items\n",
      "  speaker: 1592 items\n",
      "\n",
      "=== Subject 2, Trial 5 ===\n",
      "Number of electrodes: 135\n",
      "Movie name: black-panther\n",
      "\n",
      "Task sizes:\n",
      "  frame_brightness: 2561 items\n",
      "  global_flow: 2562 items\n",
      "  local_flow: 2562 items\n",
      "  face_num: 3544 items\n",
      "  volume: 2561 items\n",
      "  pitch: 2561 items\n",
      "  delta_volume: 2561 items\n",
      "  delta_pitch: 2561 items\n",
      "  speech: 8696 items\n",
      "  onset: 1744 items\n",
      "  gpt2_surprisal: 2561 items\n",
      "  word_length: 2562 items\n",
      "  word_gap: 2268 items\n",
      "  word_index: 1744 items\n",
      "  word_head_pos: 4808 items\n",
      "  word_part_speech: 1720 items\n",
      "  speaker: 1386 items\n",
      "\n",
      "=== Subject 2, Trial 6 ===\n",
      "Number of electrodes: 135\n",
      "Movie name: aquaman\n",
      "\n",
      "Task sizes:\n",
      "  frame_brightness: 3375 items\n",
      "  global_flow: 3375 items\n",
      "  local_flow: 3375 items\n",
      "  face_num: 4630 items\n",
      "  volume: 3375 items\n",
      "  pitch: 3375 items\n",
      "  delta_volume: 3375 items\n",
      "  delta_pitch: 3375 items\n",
      "  speech: 13500 items\n",
      "  onset: 2062 items\n",
      "  gpt2_surprisal: 3375 items\n",
      "  word_length: 3366 items\n",
      "  word_gap: 2916 items\n",
      "  word_index: 2062 items\n",
      "  word_head_pos: 5790 items\n",
      "  word_part_speech: 2160 items\n",
      "  speaker: 2662 items\n",
      "\n",
      "=== Subject 3, Trial 0 ===\n",
      "Number of electrodes: 124\n",
      "Movie name: cars-2\n",
      "\n",
      "Task sizes:\n",
      "  frame_brightness: 5379 items\n",
      "  global_flow: 5381 items\n",
      "  local_flow: 5379 items\n",
      "  face_num: 9602 items\n",
      "  volume: 5379 items\n",
      "  pitch: 5379 items\n",
      "  delta_volume: 5379 items\n",
      "  delta_pitch: 5379 items\n",
      "  speech: 3964 items\n",
      "  onset: 3964 items\n",
      "  gpt2_surprisal: 5379 items\n",
      "  word_length: 5379 items\n",
      "  word_gap: 4478 items\n",
      "  word_index: 4088 items\n",
      "  word_head_pos: 9848 items\n",
      "  word_part_speech: 3284 items\n",
      "  speaker: 5650 items\n",
      "\n",
      "=== Subject 3, Trial 1 ===\n",
      "Number of electrodes: 124\n",
      "Movie name: lotr-1\n",
      "\n",
      "Task sizes:\n",
      "  frame_brightness: 3819 items\n",
      "  global_flow: 3819 items\n",
      "  local_flow: 3820 items\n",
      "  face_num: 5614 items\n",
      "  volume: 3819 items\n",
      "  pitch: 3819 items\n",
      "  delta_volume: 3819 items\n",
      "  delta_pitch: 3819 items\n",
      "  speech: 13034 items\n",
      "  onset: 2244 items\n",
      "  gpt2_surprisal: 3819 items\n",
      "  word_length: 3819 items\n",
      "  word_gap: 3335 items\n",
      "  word_index: 2244 items\n",
      "  word_head_pos: 6642 items\n",
      "  word_part_speech: 2176 items\n",
      "  speaker: 4062 items\n",
      "\n",
      "=== Subject 3, Trial 2 ===\n",
      "Number of electrodes: 124\n",
      "Movie name: lotr-2\n",
      "\n",
      "Task sizes:\n",
      "  frame_brightness: 5313 items\n",
      "  global_flow: 5312 items\n",
      "  local_flow: 5314 items\n",
      "  face_num: 5856 items\n",
      "  volume: 5313 items\n",
      "  pitch: 5313 items\n",
      "  delta_volume: 5313 items\n",
      "  delta_pitch: 5313 items\n",
      "  speech: 14974 items\n",
      "  onset: 3420 items\n",
      "  gpt2_surprisal: 5313 items\n",
      "  word_length: 5313 items\n",
      "  word_gap: 4511 items\n",
      "  word_index: 3420 items\n",
      "  word_head_pos: 9514 items\n",
      "  word_part_speech: 3192 items\n",
      "  speaker: 2070 items\n",
      "\n",
      "=== Subject 4, Trial 0 ===\n",
      "Number of electrodes: 185\n",
      "Movie name: shrek-the-third\n",
      "\n",
      "Task sizes:\n",
      "  frame_brightness: 3387 items\n",
      "  global_flow: 3386 items\n",
      "  local_flow: 3388 items\n",
      "  face_num: 2848 items\n",
      "  volume: 3387 items\n",
      "  pitch: 3387 items\n",
      "  delta_volume: 3387 items\n",
      "  delta_pitch: 3387 items\n",
      "  speech: 6698 items\n",
      "  onset: 2112 items\n",
      "  gpt2_surprisal: 3387 items\n",
      "  word_length: 3386 items\n",
      "  word_gap: 3525 items\n",
      "  word_index: 2112 items\n",
      "  word_head_pos: 5992 items\n",
      "  word_part_speech: 2116 items\n",
      "  speaker: 3104 items\n",
      "\n",
      "=== Subject 4, Trial 1 ===\n",
      "Number of electrodes: 185\n",
      "Movie name: megamind\n",
      "\n",
      "Task sizes:\n",
      "  frame_brightness: 4141 items\n",
      "  global_flow: 4142 items\n",
      "  local_flow: 4140 items\n",
      "  face_num: 4290 items\n",
      "  volume: 4141 items\n",
      "  pitch: 4141 items\n",
      "  delta_volume: 4141 items\n",
      "  delta_pitch: 4141 items\n",
      "  speech: 3032 items\n",
      "  onset: 2898 items\n",
      "  gpt2_surprisal: 4141 items\n",
      "  word_length: 4160 items\n",
      "  word_gap: 3317 items\n",
      "  word_index: 2898 items\n",
      "  word_head_pos: 7474 items\n",
      "  word_part_speech: 2632 items\n",
      "  speaker: 5940 items\n",
      "\n",
      "=== Subject 4, Trial 2 ===\n",
      "Number of electrodes: 185\n",
      "Movie name: incredibles\n",
      "\n",
      "Task sizes:\n",
      "  frame_brightness: 2477 items\n",
      "  global_flow: 2480 items\n",
      "  local_flow: 2477 items\n",
      "  face_num: 4486 items\n",
      "  volume: 2477 items\n",
      "  pitch: 2477 items\n",
      "  delta_volume: 2477 items\n",
      "  delta_pitch: 2477 items\n",
      "  speech: 3302 items\n",
      "  onset: 1594 items\n",
      "  gpt2_surprisal: 2477 items\n",
      "  word_length: 2489 items\n",
      "  word_gap: 2931 items\n",
      "  word_index: 1594 items\n",
      "  word_head_pos: 4596 items\n",
      "  word_part_speech: 1684 items\n",
      "  speaker: 1872 items\n",
      "\n",
      "=== Subject 5, Trial 0 ===\n",
      "Number of electrodes: 140\n",
      "Movie name: fantastic-mr-fox\n",
      "\n",
      "Task sizes:\n",
      "  frame_brightness: 3953 items\n",
      "  global_flow: 3952 items\n",
      "  local_flow: 3952 items\n",
      "  face_num: 3638 items\n",
      "  volume: 3953 items\n",
      "  pitch: 3953 items\n",
      "  delta_volume: 3953 items\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m eval_name \u001b[38;5;129;01min\u001b[39;00m btbench_tasks:\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 21\u001b[0m         dataset \u001b[38;5;241m=\u001b[39m \u001b[43mBrainTreebankSubjectTrialBenchmarkDataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m            \u001b[49m\u001b[43msubject\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m            \u001b[49m\u001b[43moutput_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstart_neural_data_before_word_onset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_neural_data_before_word_onset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m            \u001b[49m\u001b[43mend_neural_data_after_word_onset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mend_neural_data_after_word_onset\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m  \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(dataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m items\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/net/vast-storage/scratch/vast/yanglab/zaho/btbench/btbench_datasets.py:149\u001b[0m, in \u001b[0;36mBrainTreebankSubjectTrialBenchmarkDataset.__init__\u001b[0;34m(self, subject, trial_id, dtype, eval_name, output_indices, start_neural_data_before_word_onset, end_neural_data_after_word_onset, lite, random_seed, allow_partial_cache, binary_tasks)\u001b[0m\n\u001b[1;32m    145\u001b[0m     all_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_words_df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_name_remapped]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# Get indices for words in top and bottom quartiles\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m label_percentiles \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([np\u001b[38;5;241m.\u001b[39mmean(all_labels \u001b[38;5;241m<\u001b[39m x) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m all_labels])\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextreme_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere((label_percentiles \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.75\u001b[39m) \u001b[38;5;241m|\u001b[39m (label_percentiles \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.25\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextreme_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy((label_percentiles[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextreme_indices] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.75\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m))\n",
      "File \u001b[0;32m/net/vast-storage/scratch/vast/yanglab/zaho/btbench/btbench_datasets.py:149\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    145\u001b[0m     all_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mall_words_df[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meval_name_remapped]\u001b[38;5;241m.\u001b[39mto_numpy()\n\u001b[1;32m    148\u001b[0m \u001b[38;5;66;03m# Get indices for words in top and bottom quartiles\u001b[39;00m\n\u001b[0;32m--> 149\u001b[0m label_percentiles \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_labels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m all_labels])\n\u001b[1;32m    150\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextreme_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere((label_percentiles \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.75\u001b[39m) \u001b[38;5;241m|\u001b[39m (label_percentiles \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0.25\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextreme_labels \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy((label_percentiles[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextreme_indices] \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.75\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m))\n",
      "File \u001b[0;32m/net/vast-storage/scratch/vast/yanglab/zaho/bfm_ic2/.venv/lib/python3.9/site-packages/numpy/core/fromnumeric.py:3504\u001b[0m, in \u001b[0;36mmean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m   3501\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3502\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m mean(axis\u001b[38;5;241m=\u001b[39maxis, dtype\u001b[38;5;241m=\u001b[39mdtype, out\u001b[38;5;241m=\u001b[39mout, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m-> 3504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_methods\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mean\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3505\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/net/vast-storage/scratch/vast/yanglab/zaho/bfm_ic2/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:106\u001b[0m, in \u001b[0;36m_mean\u001b[0;34m(a, axis, dtype, out, keepdims, where)\u001b[0m\n\u001b[1;32m    102\u001b[0m arr \u001b[38;5;241m=\u001b[39m asanyarray(a)\n\u001b[1;32m    104\u001b[0m is_float16_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m rcount \u001b[38;5;241m=\u001b[39m \u001b[43m_count_reduce_items\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m rcount \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m umr_any(rcount \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    108\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMean of empty slice.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;167;01mRuntimeWarning\u001b[39;00m, stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n",
      "File \u001b[0;32m/net/vast-storage/scratch/vast/yanglab/zaho/bfm_ic2/.venv/lib/python3.9/site-packages/numpy/core/_methods.py:72\u001b[0m, in \u001b[0;36m_count_reduce_items\u001b[0;34m(arr, axis, keepdims, where)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m where \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# no boolean mask given, calculate items according to axis\u001b[39;00m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 72\u001b[0m         axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;28mrange\u001b[39m(\u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m))\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(axis, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[1;32m     74\u001b[0m         axis \u001b[38;5;241m=\u001b[39m (axis,)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Loop through all subject-trial pairs\n",
    "for subject_id, trial_id in all_subject_trials:\n",
    "    print(f\"\\n=== Subject {subject_id}, Trial {trial_id} ===\")\n",
    "    \n",
    "    # Load the subject\n",
    "    subject = BrainTreebankSubject(subject_id, allow_corrupted=False, cache=False, dtype=torch.float32)\n",
    "    print(f\"Number of electrodes: {len(subject.electrode_labels)}\")\n",
    "\n",
    "    movie_name = btbench_config.BRAINTREEBANK_SUBJECT_TRIAL_MOVIE_NAME_MAPPING[f\"btbank{subject_id}_{trial_id}\"]\n",
    "    print(f\"Movie name: {movie_name}\")\n",
    "    \n",
    "    # Common dataset parameters\n",
    "    output_indices = False\n",
    "    start_neural_data_before_word_onset = 0  # the number of samples to start the neural data before each word onset\n",
    "    end_neural_data_after_word_onset = btbench_config.SAMPLING_RATE * 1  # the number of samples to end the neural data after each word onset -- here we use 1 second\n",
    "    \n",
    "    # Try each evaluation task\n",
    "    print(\"\\nTask sizes:\")\n",
    "    for eval_name in btbench_tasks:\n",
    "        try:\n",
    "            dataset = BrainTreebankSubjectTrialBenchmarkDataset(\n",
    "                subject, trial_id, dtype=torch.float32, eval_name=eval_name, \n",
    "                output_indices=output_indices, \n",
    "                start_neural_data_before_word_onset=start_neural_data_before_word_onset, \n",
    "                end_neural_data_after_word_onset=end_neural_data_after_word_onset\n",
    "            )\n",
    "            print(f\"  {eval_name}: {len(dataset)} items\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {eval_name}: Error - {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Subject 1, Trial 0 ===\n",
      "Number of electrodes: 130\n",
      "\n",
      "Task sizes:\n",
      "  enhanced_pitch: 3500 items\n",
      "\n",
      "=== Subject 1, Trial 1 ===\n",
      "Number of electrodes: 130\n",
      "\n",
      "Task sizes:\n",
      "  enhanced_pitch: 3500 items\n",
      "\n",
      "=== Subject 1, Trial 2 ===\n",
      "Number of electrodes: 130\n",
      "\n",
      "Task sizes:\n",
      "  enhanced_pitch: 3500 items\n",
      "\n",
      "=== Subject 2, Trial 0 ===\n",
      "Number of electrodes: 135\n",
      "\n",
      "Task sizes:\n",
      "  enhanced_pitch: 3500 items\n",
      "\n",
      "=== Subject 2, Trial 1 ===\n",
      "Number of electrodes: 135\n",
      "\n",
      "Task sizes:\n",
      "  enhanced_pitch: 3500 items\n",
      "\n",
      "=== Subject 2, Trial 2 ===\n",
      "Number of electrodes: 135\n",
      "\n",
      "Task sizes:\n",
      "  enhanced_pitch: 3500 items\n",
      "\n",
      "=== Subject 2, Trial 3 ===\n",
      "Number of electrodes: 135\n",
      "\n",
      "Task sizes:\n",
      "  enhanced_pitch: 3500 items\n",
      "\n",
      "=== Subject 2, Trial 4 ===\n",
      "Number of electrodes: 135\n",
      "\n",
      "Task sizes:\n",
      "  enhanced_pitch: 3500 items\n",
      "\n",
      "=== Subject 2, Trial 5 ===\n",
      "Number of electrodes: 135\n",
      "\n",
      "Task sizes:\n",
      "  enhanced_pitch: 2561 items\n",
      "\n",
      "=== Subject 2, Trial 6 ===\n",
      "Number of electrodes: 135\n",
      "\n",
      "Task sizes:\n",
      "  enhanced_pitch: 3375 items\n",
      "\n",
      "=== Subject 3, Trial 0 ===\n",
      "Number of electrodes: 124\n",
      "\n",
      "Task sizes:\n",
      "  enhanced_pitch: 3500 items\n",
      "\n",
      "=== Subject 3, Trial 1 ===\n",
      "Number of electrodes: 124\n",
      "\n",
      "Task sizes:\n",
      "  enhanced_pitch: 3500 items\n",
      "\n",
      "=== Subject 3, Trial 2 ===\n",
      "Number of electrodes: 124\n",
      "\n",
      "Task sizes:\n",
      "  enhanced_pitch: 3500 items\n",
      "\n",
      "=== Subject 4, Trial 0 ===\n",
      "Number of electrodes: 185\n",
      "\n",
      "Task sizes:\n",
      "  enhanced_pitch: 3387 items\n",
      "\n",
      "=== Subject 4, Trial 1 ===\n",
      "Number of electrodes: 185\n",
      "\n",
      "Task sizes:\n",
      "  enhanced_pitch: 3500 items\n",
      "\n",
      "=== Subject 4, Trial 2 ===\n",
      "Number of electrodes: 185\n",
      "\n",
      "Task sizes:\n",
      "  enhanced_pitch: 2477 items\n",
      "\n",
      "=== Subject 7, Trial 0 ===\n",
      "Number of electrodes: 240\n",
      "\n",
      "Task sizes:\n",
      "  enhanced_pitch: 3500 items\n",
      "\n",
      "=== Subject 7, Trial 1 ===\n",
      "Number of electrodes: 240\n",
      "\n",
      "Task sizes:\n",
      "  enhanced_pitch: 3500 items\n",
      "\n",
      "=== Subject 10, Trial 0 ===\n",
      "Number of electrodes: 207\n",
      "\n",
      "Task sizes:\n",
      "  enhanced_pitch: 3500 items\n",
      "\n",
      "=== Subject 10, Trial 1 ===\n",
      "Number of electrodes: 207\n",
      "\n",
      "Task sizes:\n",
      "  enhanced_pitch: 3500 items\n"
     ]
    }
   ],
   "source": [
    "# Loop through all subject-trial pairs\n",
    "for subject_id, trial_id in all_subject_trials:\n",
    "    if subject_id not in [1, 2, 3, 4, 7, 10]:\n",
    "        continue\n",
    "    print(f\"\\n=== Subject {subject_id}, Trial {trial_id} ===\")\n",
    "    \n",
    "    # Load the subject\n",
    "    subject = BrainTreebankSubject(subject_id, allow_corrupted=False, cache=False, dtype=torch.float32)\n",
    "    print(f\"Number of electrodes: {len(subject.electrode_labels)}\")\n",
    "    \n",
    "    # Common dataset parameters\n",
    "    output_indices = False\n",
    "    start_neural_data_before_word_onset = 0  # the number of samples to start the neural data before each word onset\n",
    "    end_neural_data_after_word_onset = btbench_config.SAMPLING_RATE * 1  # the number of samples to end the neural data after each word onset -- here we use 1 second\n",
    "    \n",
    "    # Try each evaluation task\n",
    "    print(\"\\nTask sizes:\")\n",
    "    for eval_name in btbench_tasks:\n",
    "        try:\n",
    "            dataset = BrainTreebankSubjectTrialBenchmarkDataset(\n",
    "                subject, trial_id, dtype=torch.float32, eval_name=eval_name, \n",
    "                output_indices=output_indices, \n",
    "                start_neural_data_before_word_onset=start_neural_data_before_word_onset, \n",
    "                end_neural_data_after_word_onset=end_neural_data_after_word_onset,\n",
    "                lite=True\n",
    "            )\n",
    "            print(f\"  {eval_name}: {len(dataset)} items\")\n",
    "        except Exception as e:\n",
    "            print(f\"  {eval_name}: Error - {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Subject 1 ===\n",
      "Number of electrodes: 130\n",
      "\n",
      "=== Subject 2 ===\n",
      "Number of electrodes: 135\n",
      "\n",
      "=== Subject 3 ===\n",
      "Number of electrodes: 124\n",
      "\n",
      "=== Subject 4 ===\n",
      "Number of electrodes: 185\n",
      "\n",
      "=== Subject 7 ===\n",
      "Number of electrodes: 240\n",
      "\n",
      "=== Subject 10 ===\n",
      "Number of electrodes: 207\n"
     ]
    }
   ],
   "source": [
    "# Loop through all subject-trial pairs\n",
    "for subject_id in [1, 2, 3, 4, 7, 10]:\n",
    "    print(f\"\\n=== Subject {subject_id} ===\")\n",
    "    \n",
    "    # Load the subject\n",
    "    subject = BrainTreebankSubject(subject_id, allow_corrupted=False, cache=False, dtype=torch.float32)\n",
    "    print(f\"Number of electrodes: {len(subject.electrode_labels)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BFM IC2 (.venv)",
   "language": "python",
   "name": "bfm_ic2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
